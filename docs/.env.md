# Documentation for xAI Backend Integration

This documentation covers a recent code change that introduces new environment variables for configuring an xAI (Grok) backend, enhances LLM (Large Language Model) backend auto-detection, and updates related configurations for better flexibility and cost-efficiency. The changes aim to allow users to seamlessly switch between different LLM providers (e.g., Anthropic, xAI, or Ollama) while prioritizing cost-effective options like xAI.

## Overview

### What This Does and Why It Matters
This change adds support for the xAI backend (powered by Grok) as an alternative to existing options like Anthropic (Claude) and Ollama. It introduces new environment variables such as `TOME_WEBHOOK_SECRET`, `XAI_API_KEY`, and `TOME_XAI_MODEL` in the `.env.example` file. These variables enable:

- **Integration with LLM backend auto-detection**: The application automatically selects an LLM backend based on which API keys are provided, falling back to a default order (Anthropic > xAI > Ollama). This reduces manual configuration and ensures the app uses the most appropriate backend.
- **Cost-efficiency**: xAI is recommended for users seeking affordable AI inference, as it offers competitive pricing compared to other providers. By setting up xAI, users can leverage its models for tasks like prompt generation without incurring high costs.
- **Enhanced security and flexibility**: Variables like `TOME_WEBHOOK_SECRET` improve webhook security, while the new setup allows for easier switching between backends based on availability or performance needs.

This update matters because it makes the application more versatile, cost-effective, and user-friendly, especially for developers working with AI-driven features in a production environment.

### Key Changes
- **.env.example**: A new file with examples for all environment variables, including the new ones for xAI.
- **config.py**: Updated the `LLM_BACKEND` logic to include "xai" in auto-detection.
- **app.py and engine.py**: Added checks and functions to handle the xAI backend, ensuring it integrates with the existing LLM workflow.

## New Environment Variables

The following variables have been added or updated in `.env.example`. They control various aspects of the application, with a focus on LLM backends.

| Variable              | Description | Default Value | Required? | Notes |
|-----------------------|-------------|---------------|-----------|--------|
| **TOME_WEBHOOK_SECRET** | A secret key for verifying GitHub webhook signatures. This enhances security by ensuring incoming webhooks are authentic. | (empty) | Optional | Set this to a strong, random string if you're using webhooks. Example: `TOME_WEBHOOK_SECRET=my_secure_secret_123`. |
| **XAI_API_KEY** | Your xAI API key, required for authenticating requests to the xAI backend (e.g., Grok models). | (empty) | Required for xAI backend | Obtain this from the xAI dashboard. It's used for cost-efficient AI inference. |
| **TOME_XAI_MODEL** | Specifies the xAI model to use (e.g., "grok-3-mini-fast"). This integrates with the LLM backend for tasks like text generation. | "grok-3-mini-fast" | Optional | Choose a model based on your needs; lighter models are faster and cheaper. |

Other variables in `.env.example` (e.g., for Anthropic or Ollama) remain unchanged but are now part of the auto-detection system.

### Parameters and Options
- **LLM Backend Options**: The `TOME_LLM_BACKEND` variable can be set to "anthropic", "xai", or "ollama". If not explicitly set, the app auto-detects based on available API keys:
  - Priority order: Anthropic (if `ANTHROPIC_API_KEY` is set) > xAI (if `XAI_API_KEY` is set) > Ollama (default, no key needed).
  - Example: If both `ANTHROPIC_API_KEY` and `XAI_API_KEY` are set, Anthropic will be chosen unless `TOME_LLM_BACKEND` is manually set to "xai".
- **Model Selection**: Variables like `TOME_XAI_MODEL` allow you to specify exact models. For xAI, options include "grok-3-mini-fast" (recommended for speed) or others available in the xAI API.

## How to Use It

### Setting Up xAI as the LLM Backend
To use xAI for cost-efficiency:
1. Obtain an xAI API key from the xAI developer portal.
2. Edit your `.env` file (based on `.env.example`) and set the relevant variables:
   - Set `XAI_API_KEY` to your actual key.
   - Optionally, set `TOME_XAI_MODEL` to your preferred model (e.g., "grok-3-mini-fast").
   - To force xAI as the backend, set `TOME_LLM_BACKEND=xai`. Otherwise, let auto-detection handle it.

**Example .env Configuration for xAI:**
```
TOME_GITHUB_TOKEN=ghp_your_token_here
TOME_WEBHOOK_SECRET=my_secure_secret_123  # For security
TOME_LLM_BACKEND=xai  # Explicitly set for xAI
XAI_API_KEY=xai-your_key_here  # Replace with your actual key
TOME_XAI_MODEL=grok-3-mini-fast  # Recommended for cost-efficiency
```

### Code Examples
Here's how these changes are used in the code:

1. **Auto-Detection in config.py**:
   ```python
   import os

   class Config:
       # LLM backend auto-detection
       LLM_BACKEND = os.getenv("TOME_LLM_BACKEND",
           "anthropic" if os.getenv("ANTHROPIC_API_KEY")
           else "xai" if os.getenv("XAI_API_KEY")
           else "ollama"
       )
       XAI_API_KEY = os.getenv("XAI_API_KEY", "")
       XAI_MODEL = os.getenv("TOME_XAI_MODEL", "grok-3-mini-fast")
   ```
   - This code checks for API keys in order and sets the backend automatically.

2. **Usage in app.py (Health Check)**:
   ```python
   async def health():
       if Config.LLM_BACKEND == "xai":
           llm_status = "configured" if Config.XAI_API_KEY else "missing_key"
           model = Config.XAI_MODEL
   ```
   - The app checks if xAI is configured and reports the status.

3. **LLM Generation in engine.py**:
   ```python
   async def llm_generate(prompt: str, json_mode: bool = False) -> str:
       if Config.LLM_BACKEND == "xai":
           return await _xai_generate(prompt, json_mode)  # Internal function for xAI calls
   ```
   - This function routes prompts to the xAI backend if selected.

To integrate this into your workflow:
- Run the app with the updated `.env` file: `python app.py`.
- Test the LLM backend by sending a prompt; monitor logs for backend selection.

## Common Patterns and Gotchas

- **Best Practices**:
  - **Prioritize Security**: Always set `TOME_WEBHOOK_SECRET` for production environments to prevent unauthorized access.
  - **Cost Management**: Use xAI for high-volume tasks due to its efficiency. Monitor your API usage via the xAI dashboard to avoid unexpected costs.
  - **Auto-Detection Order**: Remember the fallback order (Anthropic > xAI > Ollama). If you want xAI as the default, set `TOME_LLM_BACKEND=xai` explicitly.
  - **Environment File Handling**: Copy `.env.example` to `.env` and fill in your values. Use a tool like `python-dotenv` to load these variables securely.

- **Potential Issues and How to Avoid Them**:
  - **Missing API Key**: If `XAI_API_KEY` is not set, the app will skip xAI and fall back to another backend. Check logs for errors like "missing_key" and ensure your key is valid.
  - **Rate Limiting**: xAI may impose limits on requests. Handle this by implementing retry logic or switching backends dynamically.
  - **Model Compatibility**: Not all xAI models support every feature (e.g., JSON mode). Test your chosen model with sample prompts before deploying.
  - **Cross-Platform Setup**: If running Ollama locally, ensure it's accessible via the specified URL (e.g., `http://localhost:11434`). For xAI, verify network access to their API endpoints.
  - **Debugging Tip**: Add logging in `config.py` to output the detected `LLM_BACKEND` for easier troubleshooting.

This documentation ensures you can implement and benefit from the xAI integration effectively. If you encounter issues, refer to the code changes or the project's README for additional context.